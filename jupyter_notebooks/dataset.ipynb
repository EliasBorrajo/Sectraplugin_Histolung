{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler, WeightedRandomSampler, Sampler, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class ImbalancedDatasetSampler_multilabel(Sampler):\n",
    "\n",
    "\tdef __init__(self, dataset, indices=None, num_samples=None):\n",
    "\n",
    "\t\tself.indices = list(range(len(dataset))) if indices is None else indices\n",
    "\n",
    "\t\tself.num_samples = len(self.indices) if num_samples is None else num_samples\n",
    "\n",
    "\t\tlabel_to_count = {}\n",
    "\t\tfor idx in self.indices:\n",
    "\t\t\tlabel = self._get_label(dataset, idx)\n",
    "\t\t\tfor l in label:\n",
    "\t\t\t\tif l in label_to_count:\n",
    "\t\t\t\t\tlabel_to_count[l] += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlabel_to_count[l] = 1\n",
    "\n",
    "\t\tweights = []\n",
    "\n",
    "\t\tfor idx in self.indices:\n",
    "\t\t\tc = 0\n",
    "\t\t\tfor j, l in enumerate(self._get_label(dataset, idx)):\n",
    "\t\t\t\tc = c+(1/label_to_count[l])\n",
    "\t\t\t\t\n",
    "\t\t\tweights.append(c/(j+1))\n",
    "\t\tself.weights = torch.DoubleTensor(weights)\n",
    "\t\t\n",
    "\tdef _get_label(self, dataset, idx):\n",
    "\t\tlabels = np.where(dataset[idx,1:]==1)[0]\n",
    "\t\t#print(labels)\n",
    "\t\t#labels = dataset[idx,2]\n",
    "\t\treturn labels\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\treturn (self.indices[i] for i in torch.multinomial(\n",
    "\t\t\tself.weights, self.num_samples, replacement=True))\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.num_samples\n",
    "\n",
    "class Balanced_Multimodal(Sampler):\n",
    "\n",
    "\tdef __init__(self, dataset, indices=None, num_samples=None, alpha = 0.5):\n",
    "\n",
    "\t\tself.indices = list(range(len(dataset)))             if indices is None else indices\n",
    "\n",
    "\t\tself.num_samples = len(self.indices)             if num_samples is None else num_samples\n",
    "\n",
    "\t\tclass_sample_count = [0,0,0,0,0]\n",
    "\n",
    "\n",
    "\t\tclass_sample_count = np.sum(train_dataset[:,1:],axis=0)\n",
    "\n",
    "\t\tmin_class = np.argmin(class_sample_count)\n",
    "\t\tclass_sample_count = np.array(class_sample_count)\n",
    "\t\tweights = []\n",
    "\t\tfor c in class_sample_count:\n",
    "\t\t\tweights.append((c/class_sample_count[min_class]))\n",
    "\n",
    "\t\tratio = np.array(weights).astype(np.float)\n",
    "\n",
    "\t\tlabel_to_count = {}\n",
    "\t\tfor idx in self.indices:\n",
    "\t\t\tlabel = self._get_label(dataset, idx)\n",
    "\t\t\tfor l in label:\n",
    "\t\t\t\tif l in label_to_count:\n",
    "\t\t\t\t\tlabel_to_count[l] += 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tlabel_to_count[l] = 1\n",
    "\n",
    "\t\tweights = []\n",
    "\n",
    "\t\tfor idx in self.indices:\n",
    "\t\t\tc = 0\n",
    "\t\t\tfor j, l in enumerate(self._get_label(dataset, idx)):\n",
    "\t\t\t\tc = c+(1/label_to_count[l])#*ratio[l]\n",
    "\n",
    "\t\t\tweights.append(c/(j+1))\n",
    "\t\t\t#weights.append(c)\n",
    "\t\t\t\n",
    "\t\tself.weights_original = torch.DoubleTensor(weights)\n",
    "\n",
    "\t\tself.weights_uniform = np.repeat(1/self.num_samples, self.num_samples)\n",
    "\n",
    "\t\t#print(self.weights_a, self.weights_b)\n",
    "\n",
    "\t\tbeta = 1 - alpha\n",
    "\t\tself.weights = (alpha * self.weights_original) + (beta * self.weights_uniform)\n",
    "\n",
    "\n",
    "\tdef _get_label(self, dataset, idx):\n",
    "\t\tlabels = np.where(dataset[idx,1:]==1)[0]\n",
    "\t\t#print(labels)\n",
    "\t\t#labels = dataset[idx,2]\n",
    "\t\treturn labels\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\treturn (self.indices[i] for i in torch.multinomial(\n",
    "\t\t\tself.weights, self.num_samples, replacement=True))\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.num_samples\n",
    "\n",
    "\n",
    "class Dataset_instance(Dataset):\n",
    "\n",
    "\tdef __init__(self, list_IDs, mode):\n",
    "\t\tself.list_IDs = list_IDs\n",
    "\t\t#self.list_IDs = list_IDs[:,0]\n",
    "\t\t#self.list_hes = list_IDs[:,1:]\n",
    "\t\tself.mode = mode\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.list_IDs)\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t# Select sample\n",
    "\t\tID = self.list_IDs[index]\n",
    "\t\t# Load data and get label\n",
    "\t\twith open(ID, 'rb') as fin:\n",
    "\t\t\tX = pyspng.load(fin.read())\n",
    "\t\t#img.close()\n",
    "\n",
    "\t\t#k = pipeline_transform_soft(image=k)['image']\n",
    "\t\t#k = pipeline_transform(image=q)['image']\n",
    "\n",
    "\t\tk = X\t\t\t\t\n",
    "\n",
    "\t\t#k = pipeline_transform_soft(image=k)['image']\n",
    "\t\tq = pipeline_transform(image=k)['image']\n",
    "\n",
    "\t\tq = preprocess(q).type(torch.FloatTensor)\n",
    "\t\tk = preprocess(k).type(torch.FloatTensor)\n",
    "\t\t#return input_tensor\n",
    "\t\treturn k, q\n",
    "\n",
    "class Dataset_bag(Dataset):\n",
    "\n",
    "\tdef __init__(self, list_IDs, labels):\n",
    "\n",
    "\t\tself.labels = labels\n",
    "\t\tself.list_IDs = list_IDs\n",
    "\n",
    "\tdef __len__(self):\n",
    "\n",
    "\t\treturn len(self.list_IDs)\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\t# Select sample\n",
    "\t\tWSI = self.list_IDs[index]\n",
    "\n",
    "\t\treturn WSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(data, n):\n",
    "\t\n",
    "\ttrain_dataset = []\n",
    "\tvalid_dataset = []\n",
    "\t\n",
    "\tfor sample in data:\n",
    "\t\t\n",
    "\t\tfname = sample[0]\n",
    "\t\tcancer = sample[1]\n",
    "\t\thgd = sample[2]\n",
    "\t\tlgd = sample[3]\n",
    "\t\thyper = sample[4]\n",
    "\t\tnormal = sample[5]\n",
    "\t\tf = sample[6]\n",
    "\t\t\n",
    "\t\trow = [fname, cancer, hgd, lgd, hyper, normal]\n",
    "\t\t\n",
    "\t\tif (f==n):\n",
    "\t\t\t\n",
    "\t\t\tvalid_dataset.append(row)\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\t\n",
    "\t\t\ttrain_dataset.append(row)\n",
    "\t\t\t\n",
    "\ttrain_dataset = np.array(train_dataset, dtype=object)#[:30]\n",
    "\tvalid_dataset = np.array(valid_dataset, dtype=object)#[:30]\n",
    "\t\n",
    "\t#train_dataset = np.append(train_dataset, cad_data, axis=0)\n",
    "\n",
    "\treturn train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters bag\n",
    "batch_size_bag = 16\n",
    "\n",
    "\"\"\"\n",
    "sampler = ImbalancedDatasetSampler_multilabel\n",
    "params_train_bag = {'batch_size': batch_size_bag,\n",
    "\t\t'sampler': sampler(train_dataset)}\n",
    "\t\t#'shuffle': True}\n",
    "\"\"\"\n",
    "#\"\"\"\n",
    "sampler = Balanced_Multimodal\n",
    "params_train_bag = {'batch_size': batch_size_bag,\n",
    "\t\t#'sampler': sampler(train_dataset,alpha=0.25)}\n",
    "\t\t'shuffle': True}\n",
    "#\"\"\"\n",
    "\"\"\"\n",
    "sampler = Balanced_Multimodal\n",
    "params_bag_train = {'batch_size': batch_size_bag,\n",
    "\t\t'sampler': sampler(train_dataset,alpha=0.5)}\n",
    "\t\t#'shuffle': True}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "params_bag_test = {'batch_size': batch_size_bag,\n",
    "\t\t#'sampler': sampler(train_dataset)\n",
    "\t  'shuffle': True}\n",
    "\n",
    "params_bag_train_queue = {'batch_size': int(batch_size_bag*2),\n",
    "\t\t'sampler': sampler(train_dataset,alpha=0.25)}\n",
    "\t  #'shuffle': True}\n",
    "\n",
    "params_bag_test_queue = {'batch_size': int(batch_size_bag*2),\n",
    "\t\t#'sampler': sampler(train_dataset)\n",
    "\t  'shuffle': True}\n",
    "\n",
    "training_set_bag = Dataset_bag(train_dataset[:,0], train_dataset[:,1:])\n",
    "training_generator_bag = data.DataLoader(training_set_bag, **params_train_bag)\n",
    "\n",
    "#validation_set_bag = Dataset_bag(valid_dataset[:,0], valid_dataset[:,1:])\n",
    "#validation_generator_bag = data.DataLoader(validation_set_bag, **params_bag_test)\n",
    "\n",
    "training_set_bag = Dataset_bag(train_dataset[:,0], train_dataset[:,1:])\n",
    "training_generator_bag_queue = data.DataLoader(training_set_bag, **params_bag_train_queue)\n",
    "\n",
    "#validation_set_bag = Dataset_bag(valid_dataset[:,0], valid_dataset[:,1:])\n",
    "#validation_generator_bag_queue = data.DataLoader(validation_set_bag, **params_bag_test_queue)\n",
    "\n",
    "#params patches generated\n",
    "\n",
    "# Find total parameters and trainable parameters\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "\n",
    "total_trainable_params = sum(\n",
    "\tp.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lung_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6158f16043df0ed03e0f0bf66374ee44ce8098f515735805958080d32960790b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
